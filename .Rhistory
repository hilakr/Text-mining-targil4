ac[i,k] = stringsim(a[i],c[k])
}
}
ab
library(tm)
library(ggplot2)
library(lsa)
library(sets)
require(SnowballC)
library(readr)
library(RTextTools)
# Set  working directory, and place input files there
setwd("/Users/Hila/Documents/ML/textMining")
# loading the data
unzip("train.csv.zip")
train  <- read.csv("train.csv")
unzip("test.csv.zip")
test  <- read.csv("test.csv")
train$median_relevance <- factor(train$median_relevance)
#Preprocess the train data
train$query_f1 <- factor(substr(train$query,1,1),)
train$product_title_f1 <- factor(tolower(substr(train$product_title,1,1)))
train$product_description_f1 <- factor(tolower(substr(train$product_description,1,1)))
#Preprocess the test data as well
test$query_f1 <- factor(substr(test$query,1,1))
test$product_title_f1 <- factor(tolower(substr(test$product_title,1,1)))
test$product_description_f1 <- factor(tolower(substr(test$product_description,1,1)))
levels(train$query_f1) <- union(levels(train$query_f1), levels(test$query_f1))
levels(train$product_title_f1) <- union(levels(train$product_title_f1), levels(test$product_title_f1))
levels(train$product_description_f1) <- union(levels(train$product_description_f1), levels(test$product_description_f1))
levels(test$query_f1) <- union(levels(train$query_f1), levels(test$query_f1))
levels(test$product_title_f1) <- union(levels(train$product_title_f1), levels(test$product_title_f1))
levels(test$product_description_f1) <- union(levels(train$product_description_f1), levels(test$product_description_f1))
myfunction <- function(text){
df <- data.frame(text, stringsAsFactors = FALSE);
docs <- Corpus(VectorSource(df$text));
docs <- tm_map(docs, content_transformer(tolower)); # convert all text to lower case
as.character(docs[[1]]);
docs <- tm_map(docs, removePunctuation); # remove Puncturation
as.character(docs[[1]]);
docs <- tm_map(docs, removeNumbers); # remove Numbers
as.character(docs[[1]]);
docs <- tm_map(docs, removeWords, stopwords("english")); # remove common words
as.character(docs[[1]]);
docs <- tm_map(docs, stripWhitespace); # strip white space
as.character(docs[[1]]);
docs <- tm_map(docs, stemDocument); # stem the document
as.character(docs[[1]]);
query_words= tokenize(as.character(docs[[1]]));
return(query_words);
}
for (i in 1:nrow(train$query)){
words_query = myfunction(train$query[i]);
words_product_title = myfunction(train$product_title[i]);
words_product_desc = myfunction(train$product_description[i]);
words_query_test = myfunction(test$query[i]);
words_product_title_test = myfunction(test$product_title[i]);
words_product_desc_test = myfunction(test$product_description[i]);
train$myfeature[i] = set_similarity(as.set(words_query),as.set(words_product_desc));
print(train$myfeature[i])
print(test$myfeature[i])
test$myfeature[i] = set_similarity(as.set(words_query_test),as.set(words_product_desc_test));
}
library(randomForest)
model <- randomForest(median_relevance ~ query_f1+product_title_f1+product_description_f1+myfeature, data=train, ntree=3)
results <- predict(model, newdata = test)
#results = round(results)
Newsubmission = data.frame(id=test$id, prediction = results)
library(tm)
library(ggplot2)
library(lsa)
library(sets)
require(SnowballC)
library(readr)
library(RTextTools)
# Set  working directory, and place input files there
setwd("/Users/Hila/Documents/ML/textMining")
# loading the data
unzip("train.csv.zip")
train  <- read.csv("train.csv")
unzip("test.csv.zip")
test  <- read.csv("test.csv")
train$median_relevance <- factor(train$median_relevance)
#Preprocess the train data
train$query_f1 <- factor(substr(train$query,1,1),)
train$product_title_f1 <- factor(tolower(substr(train$product_title,1,1)))
train$product_description_f1 <- factor(tolower(substr(train$product_description,1,1)))
#Preprocess the test data as well
test$query_f1 <- factor(substr(test$query,1,1))
test$product_title_f1 <- factor(tolower(substr(test$product_title,1,1)))
test$product_description_f1 <- factor(tolower(substr(test$product_description,1,1)))
levels(train$query_f1) <- union(levels(train$query_f1), levels(test$query_f1))
levels(train$product_title_f1) <- union(levels(train$product_title_f1), levels(test$product_title_f1))
levels(train$product_description_f1) <- union(levels(train$product_description_f1), levels(test$product_description_f1))
levels(test$query_f1) <- union(levels(train$query_f1), levels(test$query_f1))
levels(test$product_title_f1) <- union(levels(train$product_title_f1), levels(test$product_title_f1))
levels(test$product_description_f1) <- union(levels(train$product_description_f1), levels(test$product_description_f1))
myfunction <- function(text){
df <- data.frame(text, stringsAsFactors = FALSE);
docs <- Corpus(VectorSource(df$text));
docs <- tm_map(docs, content_transformer(tolower)); # convert all text to lower case
as.character(docs[[1]]);
docs <- tm_map(docs, removePunctuation); # remove Puncturation
as.character(docs[[1]]);
docs <- tm_map(docs, removeNumbers); # remove Numbers
as.character(docs[[1]]);
docs <- tm_map(docs, removeWords, stopwords("english")); # remove common words
as.character(docs[[1]]);
docs <- tm_map(docs, stripWhitespace); # strip white space
as.character(docs[[1]]);
docs <- tm_map(docs, stemDocument); # stem the document
as.character(docs[[1]]);
query_words= tokenize(as.character(docs[[1]]));
return(query_words);
}
for (i in 1:nrow(train)){
words_query = myfunction(train$query[i]);
words_product_title = myfunction(train$product_title[i]);
words_product_desc = myfunction(train$product_description[i]);
words_query_test = myfunction(test$query[i]);
words_product_title_test = myfunction(test$product_title[i]);
words_product_desc_test = myfunction(test$product_description[i]);
train$myfeature[i] = set_similarity(as.set(words_query),as.set(words_product_desc));
print(train$myfeature[i])
print(test$myfeature[i])
test$myfeature[i] = set_similarity(as.set(words_query_test),as.set(words_product_desc_test));
}
library(randomForest)
model <- randomForest(median_relevance ~ query_f1+product_title_f1+product_description_f1+myfeature, data=train, ntree=3)
results <- predict(model, newdata = test)
#results = round(results)
Newsubmission = data.frame(id=test$id, prediction = results)
nrow(train)
rm(list=ls(all=T))
# Load
library(readr)
train <- read_csv("train.csv")
test  <- read_csv("test.csv")
# Preprocessing
## get rid of relevence_varience
train = subset(x = train, select = -relevance_variance)
## omit tupels with missing values
vec = logical(length = nrow(train))
for (j in 1:nrow(train)) if(nchar(train$product_description[j])==0) vec[j] <- F else vec[j] <- T
train = subset(train, subset = vec)
rm(vec)
## factorize median_relevance column
train$median_relevance <- factor(train$median_relevance)
## remove spaces, punctuations and digits
rgx = "[[:space:][:punct:][:digit:]]+"
for (j in seq(train$query)) train$query[[j]] = gsub(rgx," ",train$query[[j]])
for (j in seq(train$product_title)) train$product_title[[j]] = gsub(rgx," ",train$product_title[[j]])
for (j in seq(train$product_description)) train$product_description[[j]] = gsub(rgx," ",train$product_description[[j]])
for (j in seq(test$query)) test$query[[j]] = gsub(rgx," ",test$query[[j]])
library(tm)
library(ggplot2)
library(lsa)
library(sets)
require(SnowballC)
library(readr)
library(RTextTools)
# Set  working directory, and place input files there
setwd("/Users/Hila/Documents/ML/textMining")
# loading the data
unzip("train.csv.zip")
train  <- read.csv("train.csv")
unzip("test.csv.zip")
test  <- read.csv("test.csv")
train$median_relevance <- factor(train$median_relevance)
#Preprocess the train data
train$query_f1 <- factor(substr(train$query,1,1),)
train$product_title_f1 <- factor(tolower(substr(train$product_title,1,1)))
train$product_description_f1 <- factor(tolower(substr(train$product_description,1,1)))
#Preprocess the test data as well
test$query_f1 <- factor(substr(test$query,1,1))
test$product_title_f1 <- factor(tolower(substr(test$product_title,1,1)))
test$product_description_f1 <- factor(tolower(substr(test$product_description,1,1)))
levels(train$query_f1) <- union(levels(train$query_f1), levels(test$query_f1))
levels(train$product_title_f1) <- union(levels(train$product_title_f1), levels(test$product_title_f1))
levels(train$product_description_f1) <- union(levels(train$product_description_f1), levels(test$product_description_f1))
levels(test$query_f1) <- union(levels(train$query_f1), levels(test$query_f1))
levels(test$product_title_f1) <- union(levels(train$product_title_f1), levels(test$product_title_f1))
levels(test$product_description_f1) <- union(levels(train$product_description_f1), levels(test$product_description_f1))
myfunction <- function(text){
df <- data.frame(text, stringsAsFactors = FALSE);
docs <- Corpus(VectorSource(df$text));
docs <- tm_map(docs, content_transformer(tolower)); # convert all text to lower case
as.character(docs[[1]]);
docs <- tm_map(docs, removePunctuation); # remove Puncturation
as.character(docs[[1]]);
docs <- tm_map(docs, removeNumbers); # remove Numbers
as.character(docs[[1]]);
docs <- tm_map(docs, removeWords, stopwords("english")); # remove common words
as.character(docs[[1]]);
docs <- tm_map(docs, stripWhitespace); # strip white space
as.character(docs[[1]]);
docs <- tm_map(docs, stemDocument); # stem the document
as.character(docs[[1]]);
query_words= tokenize(as.character(docs[[1]]));
return(query_words);
}
for (i in 1:nrow(train)){
words_query = myfunction(train$query[i]);
words_product_title = myfunction(train$product_title[i]);
words_product_desc = myfunction(train$product_description[i]);
words_query_test = myfunction(test$query[i]);
words_product_title_test = myfunction(test$product_title[i]);
words_product_desc_test = myfunction(test$product_description[i]);
train$myfeature[i] = set_similarity(as.set(words_query),as.set(words_product_desc));
print(train$myfeature[i])
print(test$myfeature[i])
test$myfeature[i] = set_similarity(as.set(words_query_test),as.set(words_product_desc_test));
}
library(randomForest)
model <- randomForest(median_relevance ~ query_f1+product_title_f1+product_description_f1+myfeature, data=train, ntree=3)
results <- predict(model, newdata = test)
#results = round(results)
Newsubmission = data.frame(id=test$id, prediction = results)
library(tm)
library(ggplot2)
library(lsa)
library(sets)
require(SnowballC)
library(readr)
library(RTextTools)
# Set  working directory, and place input files there
setwd("/Users/Hila/Documents/ML/textMining")
# loading the data
unzip("train.csv.zip")
train  <- read.csv("train.csv")
unzip("test.csv.zip")
test  <- read.csv("test.csv")
train$median_relevance <- factor(train$median_relevance)
#Preprocess the train data
train$query_f1 <- factor(substr(train$query,1,1),)
train$product_title_f1 <- factor(tolower(substr(train$product_title,1,1)))
train$product_description_f1 <- factor(tolower(substr(train$product_description,1,1)))
#Preprocess the test data as well
test$query_f1 <- factor(substr(test$query,1,1))
test$product_title_f1 <- factor(tolower(substr(test$product_title,1,1)))
test$product_description_f1 <- factor(tolower(substr(test$product_description,1,1)))
levels(train$query_f1) <- union(levels(train$query_f1), levels(test$query_f1))
levels(train$product_title_f1) <- union(levels(train$product_title_f1), levels(test$product_title_f1))
levels(train$product_description_f1) <- union(levels(train$product_description_f1), levels(test$product_description_f1))
levels(test$query_f1) <- union(levels(train$query_f1), levels(test$query_f1))
levels(test$product_title_f1) <- union(levels(train$product_title_f1), levels(test$product_title_f1))
levels(test$product_description_f1) <- union(levels(train$product_description_f1), levels(test$product_description_f1))
myfunction <- function(text){
df <- data.frame(text, stringsAsFactors = FALSE);
docs <- Corpus(VectorSource(df$text));
docs <- tm_map(docs, content_transformer(tolower)); # convert all text to lower case
as.character(docs[[1]]);
docs <- tm_map(docs, removePunctuation); # remove Puncturation
as.character(docs[[1]]);
docs <- tm_map(docs, removeNumbers); # remove Numbers
as.character(docs[[1]]);
docs <- tm_map(docs, removeWords, stopwords("english")); # remove common words
as.character(docs[[1]]);
docs <- tm_map(docs, stripWhitespace); # strip white space
as.character(docs[[1]]);
docs <- tm_map(docs, stemDocument); # stem the document
as.character(docs[[1]]);
query_words= tokenize(as.character(docs[[1]]));
return(query_words);
}
for (i in 1:nrow(train)){
words_query = myfunction(train$query[i]);
words_product_title = myfunction(train$product_title[i]);
words_product_desc = myfunction(train$product_description[i]);
words_query_test = myfunction(test$query[i]);
words_product_title_test = myfunction(test$product_title[i]);
words_product_desc_test = myfunction(test$product_description[i]);
train$myfeature[i] = set_similarity(as.set(words_query),as.set(words_product_desc));
print(train$myfeature[i])
print(test$myfeature[i])
test$myfeature[i] = set_similarity(as.set(words_query_test),as.set(words_product_desc_test));
}
levels(train$myfeature) <- union(levels(train$myfeature), levels(test$myfeature))
levels(test$myfeature) <- union(levels(train$myfeature), levels(test$myfeature))
library(randomForest)
model <- randomForest(median_relevance ~ query_f1+product_title_f1+product_description_f1+myfeature, data=train, ntree=3)
results <- predict(model, newdata = test)
#results = round(results)
Newsubmission = data.frame(id=test$id, prediction = results)
library(tm)
library(ggplot2)
library(lsa)
library(sets)
require(SnowballC)
library(readr)
library(RTextTools)
# Set  working directory, and place input files there
setwd("/Users/Hila/Documents/ML/textMining")
# loading the data
unzip("train.csv.zip")
train  <- read.csv("train.csv")
unzip("test.csv.zip")
test  <- read.csv("test.csv")
train$median_relevance <- factor(train$median_relevance)
#Preprocess the train data
train$query_f1 <- factor(substr(train$query,1,1),)
train$product_title_f1 <- factor(tolower(substr(train$product_title,1,1)))
train$product_description_f1 <- factor(tolower(substr(train$product_description,1,1)))
#Preprocess the test data as well
test$query_f1 <- factor(substr(test$query,1,1))
test$product_title_f1 <- factor(tolower(substr(test$product_title,1,1)))
test$product_description_f1 <- factor(tolower(substr(test$product_description,1,1)))
levels(train$query_f1) <- union(levels(train$query_f1), levels(test$query_f1))
levels(train$product_title_f1) <- union(levels(train$product_title_f1), levels(test$product_title_f1))
levels(train$product_description_f1) <- union(levels(train$product_description_f1), levels(test$product_description_f1))
levels(test$query_f1) <- union(levels(train$query_f1), levels(test$query_f1))
levels(test$product_title_f1) <- union(levels(train$product_title_f1), levels(test$product_title_f1))
levels(test$product_description_f1) <- union(levels(train$product_description_f1), levels(test$product_description_f1))
myfunction <- function(text){
df <- data.frame(text, stringsAsFactors = FALSE);
docs <- Corpus(VectorSource(df$text));
docs <- tm_map(docs, content_transformer(tolower)); # convert all text to lower case
as.character(docs[[1]]);
docs <- tm_map(docs, removePunctuation); # remove Puncturation
as.character(docs[[1]]);
docs <- tm_map(docs, removeNumbers); # remove Numbers
as.character(docs[[1]]);
docs <- tm_map(docs, removeWords, stopwords("english")); # remove common words
as.character(docs[[1]]);
docs <- tm_map(docs, stripWhitespace); # strip white space
as.character(docs[[1]]);
docs <- tm_map(docs, stemDocument); # stem the document
as.character(docs[[1]]);
query_words= tokenize(as.character(docs[[1]]));
return(query_words);
}
for (i in 1:nrow(train)){
print(i);
words_query = myfunction(train$query[i]);
words_product_title = myfunction(train$product_title[i]);
words_product_desc = myfunction(train$product_description[i]);
words_query_test = myfunction(test$query[i]);
words_product_title_test = myfunction(test$product_title[i]);
words_product_desc_test = myfunction(test$product_description[i]);
train$myfeature[i] = set_similarity(as.set(words_query),as.set(words_product_desc));
print(train$myfeature[i])
print(test$myfeature[i])
test$myfeature[i] = set_similarity(as.set(words_query_test),as.set(words_product_desc_test));
}
levels(train$myfeature) <- union(levels(train$myfeature), levels(test$myfeature))
levels(test$myfeature) <- union(levels(train$myfeature), levels(test$myfeature))
library(randomForest)
model <- randomForest(median_relevance ~ query_f1+product_title_f1+product_description_f1+myfeature, data=train, ntree=3)
results <- predict(model, newdata = test)
#results = round(results)
Newsubmission = data.frame(id=test$id, prediction = results)
library(tm)
library(ggplot2)
library(lsa)
library(sets)
require(SnowballC)
library(readr)
library(RTextTools)
# Set  working directory, and place input files there
setwd("/Users/Hila/Documents/ML/textMining")
# loading the data
unzip("train.csv.zip")
train  <- read.csv("train.csv")
unzip("test.csv.zip")
test  <- read.csv("test.csv")
train$median_relevance <- factor(train$median_relevance)
#Preprocess the train data
train$query_f1 <- factor(substr(train$query,1,1),)
train$product_title_f1 <- factor(tolower(substr(train$product_title,1,1)))
train$product_description_f1 <- factor(tolower(substr(train$product_description,1,1)))
#Preprocess the test data as well
test$query_f1 <- factor(substr(test$query,1,1))
test$product_title_f1 <- factor(tolower(substr(test$product_title,1,1)))
test$product_description_f1 <- factor(tolower(substr(test$product_description,1,1)))
levels(train$query_f1) <- union(levels(train$query_f1), levels(test$query_f1))
levels(train$product_title_f1) <- union(levels(train$product_title_f1), levels(test$product_title_f1))
levels(train$product_description_f1) <- union(levels(train$product_description_f1), levels(test$product_description_f1))
levels(test$query_f1) <- union(levels(train$query_f1), levels(test$query_f1))
levels(test$product_title_f1) <- union(levels(train$product_title_f1), levels(test$product_title_f1))
levels(test$product_description_f1) <- union(levels(train$product_description_f1), levels(test$product_description_f1))
myfunction <- function(text){
df <- data.frame(text, stringsAsFactors = FALSE);
docs <- Corpus(VectorSource(df$text));
docs <- tm_map(docs, content_transformer(tolower)); # convert all text to lower case
as.character(docs[[1]]);
docs <- tm_map(docs, removePunctuation); # remove Puncturation
as.character(docs[[1]]);
docs <- tm_map(docs, removeNumbers); # remove Numbers
as.character(docs[[1]]);
docs <- tm_map(docs, removeWords, stopwords("english")); # remove common words
as.character(docs[[1]]);
docs <- tm_map(docs, stripWhitespace); # strip white space
as.character(docs[[1]]);
docs <- tm_map(docs, stemDocument); # stem the document
as.character(docs[[1]]);
query_words= tokenize(as.character(docs[[1]]));
return(query_words);
}
for (i in 1:nrow(train)){
print(i);
words_query = myfunction(train$query[i]);
words_product_title = myfunction(train$product_title[i]);
words_product_desc = myfunction(train$product_description[i]);
words_query_test = myfunction(test$query[i]);
words_product_title_test = myfunction(test$product_title[i]);
words_product_desc_test = myfunction(test$product_description[i]);
train$myfeature[i] = set_similarity(as.set(words_query),as.set(words_product_desc));
print(train$myfeature[i])
print(test$myfeature[i])
test$myfeature[i] = set_similarity(as.set(words_query_test),as.set(words_product_desc_test));
}
levels(train$myfeature) <- union(levels(train$myfeature), levels(test$myfeature))
levels(test$myfeature) <- union(levels(train$myfeature), levels(test$myfeature))
library(randomForest)
model <- randomForest(median_relevance ~ query_f1+product_title_f1+product_description_f1+myfeature, data=train, ntree=3)
results <- predict(model, newdata = test)
#results = round(results)
Newsubmission = data.frame(id=test$id, prediction = results)
---
title: "TextMining"
author: "HilaReut"
date: "11 June 2016"
output: pdf_document
---
```{r setup, include=FALSE}
#Install required packages
library(tm)
library(ggplot2)
library(lsa)
library(sets)
require(SnowballC)
library(readr)
library(RTextTools)
library(base)
# Set  working directory, and place input files there
setwd("/Users/Hila/Documents/ML/NewTextMining")
```
<h1>Crowdflower Search Results Relevance</h1>
This task was to predict the relevance of search results from eCommerce sites.
Our first approach was to organize the data:
We took the words from query,product_title,product_description fields and we remove unnecessary data like spaces, puncturation ,common words and extra and we tried to get the similarity between query-title, query-description.
but unfortuntally we didn't get high score . so we though to use the familier tf/idf to improve the prediction.
We use the example from the ppt "TextMining1".
We needed to think how to arrange the data so we can use this algorithm.
each word was word from the query words .
each document was row in our data .
we run the tf/idf for words from the query but we look for them in the words from the title/description.
we took the higher score we get from the tf/idf.This improve our score .
In addition we used the feature from the Accompanied Codeץ
<h3>Loading the data</h3>
```{r cars}
# Loading the data
unzip("train.csv.zip")
train  <- read.csv("train.csv")
unzip("test.csv.zip")
test  <- read.csv("test.csv")
```
###Preprocessing
```{r }
train$median_relevance <- factor(train$median_relevance)
```
###Extract your smart features
####First feature - each textual attribute is truncated to its first character.
```{r}
#Preprocess the train data
train$query_f1 <- factor(substr(train$query,1,1),)
train$product_title_f1 <- factor(tolower(substr(train$product_title,1,1)))
train$product_description_f1 <- factor(tolower(substr(train$product_description,1,1)))
#Preprocess the test data as well
test$query_f1 <- factor(substr(test$query,1,1))
test$product_title_f1 <- factor(tolower(substr(test$product_title,1,1)))
test$product_description_f1 <- factor(tolower(substr(test$product_description,1,1)))
levels(train$query_f1) <- union(levels(train$query_f1), levels(test$query_f1))
levels(train$product_title_f1) <- union(levels(train$product_title_f1), levels(test$product_title_f1))
levels(train$product_description_f1) <- union(levels(train$product_description_f1), levels(test$product_description_f1))
levels(test$query_f1) <- union(levels(train$query_f1), levels(test$query_f1))
levels(test$product_title_f1) <- union(levels(train$product_title_f1), levels(test$product_title_f1))
levels(test$product_description_f1) <- union(levels(train$product_description_f1), levels(test$product_description_f1))
```
###Second Feature
####Similarity between two groups of words. 1:query-product_title; 2:query-product_description.
####using set_similarity(A, B) function.
```{r warning=FALSE}
myfunction <- function(text){
df <- data.frame(text, stringsAsFactors = FALSE);
docs <- Corpus(VectorSource(df$text));
docs <- tm_map(docs, content_transformer(tolower)); # convert all text to lower case
as.character(docs[[1]]);
docs <- tm_map(docs, removePunctuation); # remove Puncturation
as.character(docs[[1]]);
docs <- tm_map(docs, removeNumbers); # remove Numbers
as.character(docs[[1]]);
docs <- tm_map(docs, removeWords, stopwords("english")); # remove common words
as.character(docs[[1]]);
docs <- tm_map(docs, stripWhitespace); # strip white space
as.character(docs[[1]]);
docs <- tm_map(docs, stemDocument); # stem the document
as.character(docs[[1]]);
query_words = scan_tokenizer(as.character(docs[[1]]));
return(query_words);
}
train$myfeature <- 0;
test$myfeature <- 0;
train$myfeature_title <- 0;
test$myfeature_title <-0;
for (i in 1:nrow(train)){
words_query = myfunction(train$query[i]);
words_product_title = myfunction(train$product_title[i]);
words_product_desc = myfunction(train$product_description[i]);
train$myfeature[i] = set_similarity(as.set(words_query),as.set(words_product_desc));
train$myfeature_title[i] = set_similarity(as.set(words_query),as.set(words_product_title));
}
